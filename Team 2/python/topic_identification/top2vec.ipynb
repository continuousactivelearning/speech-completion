{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77d504",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 22:12:56,170 - top2vec - INFO - Pre-processing documents for training\n",
      "C:\\Users\\Victus\\Biswashreya\\Speech-Completition-Prediction\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "2025-06-26 22:12:56,182 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Top2Vec on 65 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 22:13:17,184 - top2vec - INFO - Creating joint document/word embedding\n",
      "2025-06-26 22:13:17,924 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "C:\\Users\\Victus\\Biswashreya\\Speech-Completition-Prediction\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-06-26 22:13:31,974 - top2vec - INFO - Finding dense areas of documents\n",
      "C:\\Users\\Victus\\Biswashreya\\Speech-Completition-Prediction\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Victus\\Biswashreya\\Speech-Completition-Prediction\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-06-26 22:13:31,979 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m texts = df[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m].tolist()\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining Top2Vec on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[43mTop2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlearn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m topic_words, word_scores, topic_nums = model.get_topics()\n\u001b[32m     18\u001b[39m n_docs_per_topic = \u001b[32m3\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Biswashreya\\Speech-Completition-Prediction\\.venv\\Lib\\site-packages\\top2vec\\top2vec.py:784\u001b[39m, in \u001b[36mTop2Vec.__init__\u001b[39m\u001b[34m(self, documents, contextual_top2vec, c_top2vec_smoothing_window, min_count, topic_merge_delta, ngram_vocab, ngram_vocab_args, embedding_model, embedding_model_path, embedding_batch_size, split_documents, document_chunker, chunk_length, max_num_chunks, chunk_overlap_ratio, chunk_len_coverage_ratio, sentencizer, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, umap_args, gpu_umap, hdbscan_args, gpu_hdbscan, index_topics, verbose)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28mself\u001b[39m.serialized_topic_index = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    782\u001b[39m \u001b[38;5;28mself\u001b[39m.topics_indexed = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mumap_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mumap_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mhdbscan_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhdbscan_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtopic_merge_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopic_merge_delta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mgpu_umap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgpu_umap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mgpu_hdbscan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgpu_hdbscan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mindex_topics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_topics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mcontextual_top2vec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontextual_top2vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mc_top2vec_smoothing_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc_top2vec_smoothing_window\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[38;5;66;03m# initialize document indexing variables\u001b[39;00m\n\u001b[32m    794\u001b[39m \u001b[38;5;28mself\u001b[39m.document_index = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Biswashreya\\Speech-Completition-Prediction\\.venv\\Lib\\site-packages\\top2vec\\top2vec.py:1573\u001b[39m, in \u001b[36mTop2Vec.compute_topics\u001b[39m\u001b[34m(self, umap_args, hdbscan_args, topic_merge_delta, gpu_umap, gpu_hdbscan, index_topics, contextual_top2vec, c_top2vec_smoothing_window)\u001b[39m\n\u001b[32m   1570\u001b[39m logger.info(\u001b[33m'\u001b[39m\u001b[33mFinding topics\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1572\u001b[39m \u001b[38;5;66;03m# create topic vectors\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1573\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_topic_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[38;5;66;03m# deduplicate topics\u001b[39;00m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28mself\u001b[39m._deduplicate_topics(topic_merge_delta)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Biswashreya\\Speech-Completition-Prediction\\.venv\\Lib\\site-packages\\top2vec\\top2vec.py:1061\u001b[39m, in \u001b[36mTop2Vec._create_topic_vectors\u001b[39m\u001b[34m(self, cluster_labels)\u001b[39m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m -\u001b[32m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m unique_labels:\n\u001b[32m   1059\u001b[39m     unique_labels.remove(-\u001b[32m1\u001b[39m)\n\u001b[32m   1060\u001b[39m \u001b[38;5;28mself\u001b[39m.topic_vectors = \u001b[38;5;28mself\u001b[39m._l2_normalize(\n\u001b[32m-> \u001b[39m\u001b[32m1061\u001b[39m     \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdocument_vectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_labels\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[43m              \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43munique_labels\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Biswashreya\\Speech-Completition-Prediction\\.venv\\Lib\\site-packages\\numpy\\core\\shape_base.py:289\u001b[39m, in \u001b[36mvstack\u001b[39m\u001b[34m(tup, dtype, casting)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    288\u001b[39m     arrs = [arrs]\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from top2vec import Top2Vec\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../clustering/intermediate_data/clustered_embeddings.csv\")\n",
    "df = df[df[\"text\"].notnull()].reset_index(drop=True)\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "print(f\"Training Top2Vec on {len(texts)} documents...\")\n",
    "\n",
    "\n",
    "model = Top2Vec(texts, speed=\"learn\", workers=4)\n",
    "\n",
    "topic_words, word_scores, topic_nums = model.get_topics()\n",
    "n_docs_per_topic = 3\n",
    "\n",
    "used_doc_ids = set()  \n",
    "csv_data = []\n",
    "\n",
    "for topic_id, (words, scores, topic_num) in enumerate(zip(topic_words, word_scores, topic_nums)):\n",
    "    # Search more than needed to allow filtering overlaps\n",
    "    docs, doc_scores, doc_ids = model.search_documents_by_topic(\n",
    "        topic_num=topic_num,\n",
    "        num_docs=n_docs_per_topic * 5  \n",
    "    )\n",
    "\n",
    "    selected_docs = []\n",
    "    selected_ids = []\n",
    "\n",
    "    for doc, doc_id in zip(docs, doc_ids):\n",
    "        if doc_id not in used_doc_ids:\n",
    "            selected_docs.append(doc)\n",
    "            selected_ids.append(doc_id)\n",
    "            used_doc_ids.add(doc_id)\n",
    "        if len(selected_docs) == n_docs_per_topic:\n",
    "            break\n",
    "\n",
    "    if not selected_docs:\n",
    "        continue  \n",
    "\n",
    "    csv_data.append({\n",
    "        \"topic_num\": topic_num,\n",
    "        \"top_words\": \", \".join(words),\n",
    "        \"word_scores\": \", \".join([f\"{s:.4f}\" for s in scores]),\n",
    "        \"representative_docs\": \" ||| \".join(selected_docs)\n",
    "    })\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame(csv_data)\n",
    "output_df.to_csv(\"top2vec_clustered_topics.csv\", index=False)\n",
    "\n",
    "print(\"✅ Topics and representative documents saved to top2vec_clustered_topics.csv (without overlap)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0148939-f292-4bfe-8526-0380e08a9af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(speechenv)",
   "language": "python",
   "name": "speechenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
