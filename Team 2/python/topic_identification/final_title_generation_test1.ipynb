{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86b9f8f8-9139-438e-aeca-166373f40299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 210.json:   5%|████▋                                                                                  | 17/314 [00:25<19:23,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:55] → Graph Theory: Commonality and Diversity in Graphs and Handshake Problems\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 210.json:   6%|████▉                                                                                  | 18/314 [00:30<20:28,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:59] → Clue to Landscape Analogies in Marriage Dynamics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 210.json:  37%|███████████████████████████████▎                                                    | 117/314 [02:28<1:03:26, 19.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2:05] → Seven Handshakes Graph Theory Lecture Title.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 210.json:  87%|██████████████████████████████████████████████████████████████████████████▍           | 272/314 [05:02<17:37, 25.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57:14] → \"Euler's Theorems on Graph Theory and Eulerian Cycles\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 210.json:  93%|███████████████████████████████████████████████████████████████████████████████▋      | 291/314 [05:33<02:16,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5:00] → \"Reflexive Transitive Partition Vertex Graph Path Equivalence\"\n",
      "\n",
      "(Note: This summary adheres to the rules set by avoiding complete sentences and using only two to five words. It doesn't repeat keywords exactly but uses synonyms or related terms, providing a concise title for an expert summarizer.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 210.json:  95%|█████████████████████████████████████████████████████████████████████████████████▎    | 297/314 [05:47<01:08,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60:01] → \"Marital Harmony at Social Gatherings: Understanding Interactions and Decisions Among Couples and Singles\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 210.json:  95%|█████████████████████████████████████████████████████████████████████████████████▌    | 298/314 [05:55<01:15,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60:20] → Euler's Theorems on Connected Graph Vertex Counting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 210.json:  95%|█████████████████████████████████████████████████████████████████████████████████▉    | 299/314 [06:11<01:45,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6:11] → Cycle Music Session of the Day Proofed?\n",
      "\n",
      "(Note: This response adheres to all constraints by using synonyms and related concepts that can be tied back to the original keywords without directly repeating them or resorting to complete sentences.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 210.json: 100%|██████████████████████████████████████████████████████████████████████████████████████| 314/314 [06:12<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done! Titles saved to incremental_topic_titles.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from hashlib import md5\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_CSV = \"../clustering/intermediate_data/embeddings_added.csv\"\n",
    "OUTPUT_CSV = \"incremental_topic_titles.csv\"\n",
    "TARGET_FILE = \"210.json\"\n",
    "\n",
    "OLLAMA_MODEL = \"phi3:mini\"\n",
    "TOP_N_KEYWORDS = 5\n",
    "SIMILARITY_THRESHOLD = 0.3\n",
    "MAX_INPUT_TOKENS = 1000\n",
    "\n",
    "# --- SETUP ---\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "kw_model = KeyBERT(model=embedding_model)\n",
    "title_cache = {}\n",
    "\n",
    "def parse_embedding(embedding_str):\n",
    "    return np.array(eval(embedding_str))\n",
    "\n",
    "def extract_keywords(text, top_n=TOP_N_KEYWORDS):\n",
    "    keywords = kw_model.extract_keywords(text, top_n=top_n, stop_words=\"english\")\n",
    "    return [kw for kw, _ in keywords]\n",
    "\n",
    "def compute_similarity(vec1, vec2):\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "def generate_title(keywords):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert summarizer. Your task is to generate a concise TITLE (maximum 5 words) for a lecture topic based on the provided keywords.\n",
    "\n",
    "Avoid using full sentences or punctuation unless necessary. Use keywords as hints — do not repeat them directly.\n",
    "\n",
    "TITLE FORMAT:\n",
    "- 2 to 5 words only\n",
    "- No complete sentences\n",
    "- No explanations\n",
    "\n",
    "Keywords: {', '.join(keywords)}\n",
    "\n",
    "Return ONLY the title.\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\"model\": OLLAMA_MODEL, \"prompt\": prompt, \"stream\": False}\n",
    "        )\n",
    "        return response.json().get(\"response\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[Error generating title] {e}\")\n",
    "        return f\"Error - {type(e).__name__}\"\n",
    "\n",
    "def get_cached_title(keywords):\n",
    "    key = md5(' '.join(keywords).encode()).hexdigest()\n",
    "    if key in title_cache:\n",
    "        return title_cache[key]\n",
    "    title = generate_title(keywords)\n",
    "    title_cache[key] = title\n",
    "    return title\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df = df[df[\"file\"] == TARGET_FILE]\n",
    "df = df[df[\"text\"].notnull() & df[\"embedding\"].notnull()]\n",
    "df = df.sort_values(\"start\").reset_index(drop=True)\n",
    "\n",
    "# --- PROCESS ---\n",
    "results = []\n",
    "current_topic_vec = None\n",
    "current_topic_keywords = []\n",
    "topic_start = None\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {TARGET_FILE}\"):\n",
    "    text = row[\"text\"].strip()\n",
    "    emb = parse_embedding(row[\"embedding\"])\n",
    "\n",
    "    if current_topic_vec is None:\n",
    "        topic_start = row[\"start\"]\n",
    "        current_topic_vec = emb\n",
    "        current_topic_keywords = extract_keywords(text)\n",
    "        continue\n",
    "\n",
    "    similarity = compute_similarity(current_topic_vec, emb)\n",
    "\n",
    "    if similarity >= SIMILARITY_THRESHOLD:\n",
    "        current_topic_vec = (current_topic_vec + emb) / 2 \n",
    "        current_topic_keywords.extend(extract_keywords(text))\n",
    "    else:\n",
    "        title = get_cached_title(current_topic_keywords)\n",
    "        print(f\"[{row['start']}] → {title}\")\n",
    "        results.append({\n",
    "            \"file\": TARGET_FILE,\n",
    "            \"start\": topic_start,\n",
    "            \"title\": title\n",
    "        })\n",
    "\n",
    "        # Start new topic\n",
    "        topic_start = row[\"start\"]\n",
    "        current_topic_vec = emb\n",
    "        current_topic_keywords = extract_keywords(text)\n",
    "\n",
    "# Final topic\n",
    "if current_topic_vec is not None:\n",
    "    title = get_cached_title(current_topic_keywords)\n",
    "    results.append({\n",
    "        \"file\": TARGET_FILE,\n",
    "        \"start\": topic_start,\n",
    "        \"title\": title\n",
    "    })\n",
    "\n",
    "# --- SAVE ---\n",
    "pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\n✅ Done! Titles saved to {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fe89bda-ef9c-491a-a27c-2309f6fe0b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Theory and Function Representations in Diagrams\n",
      "Seven Linked Edges Clue Zero Vertex Connection\n",
      "Graph Pairing Dynamics\n",
      "Graph Isomorphism Problem in Mathematical Structures\n",
      "Bisected Isomorphic Pair Vertex Sets Example\n",
      "Bisected Implied Isomorphism Conditions in Mapping Amplitudes and Vertices\n",
      "Isomorphic Graph Preservation Mean Condition Examples\n",
      "Graph Symmetry and Adjacency in Triangles\n",
      "Graph Isomorphism Trick\n",
      "Mathematical Preservation of Seven Edges in RS Map Objects\n",
      "Graph Isomorphism in Planar and Cubic Graphs with Eyesight Analogy\n",
      "Graph Isomorphism Subsets Power Thinking\n",
      "Inner and Outer Circle Isomorphism Possible?\n",
      "Easy Graph Isomorphisms and Peterson Drawings\n",
      "Write Representation Graphs Matrix Adjacency Vertex Connectedness\n",
      "Symmetric Graph Lengths List Example\n",
      "Vertex Representation in Graph Theory\n",
      "Sum of Degrees in Graphs Turns Key Case Study\n",
      "Texas Vertex Counting Summation\n",
      "Degree Equal Proof Cycle Graph Regularity Vertex Connectedness\n",
      "Consecutive Paths in Graph Theory\n",
      "Valid Adjacent Sequences in Graph Theory\n",
      "Nodes in Mathematical Networks\n",
      "Eulerian Cycle Vertex Requirement\n",
      "Euler's Konigsberg Bridge Problem Solution\n",
      "Euler's Work on Graph Traversal Problems\n",
      "Repeated Paths in Connected Graphs\n",
      "Write Path Definition Automatic Implication Transitivity Connection Sequence Join Walk Reflexivity Start Mean\n",
      "Class and Rivers Equivalence Partitioning\n",
      "Couple Meeting Rules Discussion\n",
      "Euler's Marriage Theorem in Graph Theory Session Today\n",
      "Answers in Handshakes and Present Interactions Among People\n",
      "\n",
      "Done! Titles saved to incremental_topic_titles.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from hashlib import md5\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_CSV = \"../clustering/intermediate_data/embeddings_added.csv\"\n",
    "OUTPUT_CSV = \"incremental_topic_titles.csv\"\n",
    "TARGET_FILE = \"210.json\"\n",
    "\n",
    "OLLAMA_MODEL = \"phi3:mini\"\n",
    "TOP_N_KEYWORDS = 5\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "MAX_INPUT_TOKENS = 1000\n",
    "\n",
    "# --- SETUP ---\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "kw_model = KeyBERT(model=embedding_model)\n",
    "title_cache = {}\n",
    "\n",
    "def parse_embedding(embedding_str):\n",
    "    return np.array(eval(embedding_str))\n",
    "\n",
    "def extract_keywords(text, top_n=TOP_N_KEYWORDS):\n",
    "    keywords = kw_model.extract_keywords(text, top_n=top_n, stop_words=\"english\")\n",
    "    return [kw for kw, _ in keywords]\n",
    "\n",
    "def compute_similarity(vec1, vec2):\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "def generate_title(keywords):\n",
    "    prompt = f\"\"\"You are a title generator.\n",
    "\n",
    "Given the following keywords, generate a concise lecture topic title.\n",
    "\n",
    "- Title must be 2 to 5 words\n",
    "- No punctuation\n",
    "- No complete sentences\n",
    "- No repetition of keywords\n",
    "- No quotes, no explanations\n",
    "- Do not write anything else\n",
    "- Just the title\n",
    "\n",
    "Keywords: {', '.join(keywords)}\n",
    "Title:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": OLLAMA_MODEL,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\": {\n",
    "                    \"temperature\": 0.2,\n",
    "                    \"num_predict\": 20,  # Limit output\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        raw = response.json().get(\"response\", \"\").strip()\n",
    "\n",
    "        # --- Force cleanup (remove explanations etc.) ---\n",
    "        first_line = raw.splitlines()[0]\n",
    "        clean = first_line.strip().strip('\"').strip(\"'\").strip(\"`\")\n",
    "        return clean\n",
    "    except Exception as e:\n",
    "        print(f\"[Error generating title] {e}\")\n",
    "        return f\"Error - {type(e).__name__}\"\n",
    "\n",
    "\n",
    "def get_cached_title(keywords):\n",
    "    key = md5(' '.join(sorted(keywords)).encode()).hexdigest()\n",
    "    if key in title_cache:\n",
    "        return title_cache[key]\n",
    "    title = generate_title(keywords)\n",
    "    title_cache[key] = title\n",
    "    return title\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df = df[df[\"file\"] == TARGET_FILE]\n",
    "df = df[df[\"text\"].notnull() & df[\"embedding\"].notnull()]\n",
    "df = df.sort_values(\"start\").reset_index(drop=True)\n",
    "\n",
    "# --- PROCESS ---\n",
    "results = []\n",
    "i = 0\n",
    "n = len(df)\n",
    "\n",
    "while i < n:\n",
    "    # Step 1: Start new topic with 5-chunk window\n",
    "    seed_rows = df.iloc[i:i+5]\n",
    "    topic_start = seed_rows.iloc[0][\"start\"]\n",
    "\n",
    "    topic_keywords = set()\n",
    "    topic_vec = None\n",
    "\n",
    "    for _, row in seed_rows.iterrows():\n",
    "        text = row[\"text\"].strip()\n",
    "        emb = parse_embedding(row[\"embedding\"])\n",
    "        keywords = extract_keywords(text)\n",
    "        topic_keywords.update(keywords)\n",
    "        topic_vec = emb if topic_vec is None else (topic_vec + emb) / 2\n",
    "\n",
    "    title = get_cached_title(list(topic_keywords))\n",
    "    print(title)\n",
    "    results.append({\n",
    "        \"file\": TARGET_FILE,\n",
    "        \"start\": topic_start,\n",
    "        \"title\": title\n",
    "    })\n",
    "\n",
    "    i += 5  # move past the initial seed\n",
    "\n",
    "    # Step 2: Process the rest\n",
    "    while i < n:\n",
    "        row = df.iloc[i]\n",
    "        text = row[\"text\"].strip()\n",
    "        emb = parse_embedding(row[\"embedding\"])\n",
    "        similarity = compute_similarity(topic_vec, emb)\n",
    "\n",
    "        if similarity >= SIMILARITY_THRESHOLD:\n",
    "            # Add keywords and update topic vector\n",
    "            keywords = extract_keywords(text)\n",
    "            topic_keywords.update(keywords)\n",
    "            topic_vec = (topic_vec + emb) / 2\n",
    "            i += 1\n",
    "        else:\n",
    "            # Not similar — create a new topic from next 5\n",
    "            break  # exit to outer while loop to process next topic\n",
    "\n",
    "# --- SAVE ---\n",
    "pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\nDone! Titles saved to {OUTPUT_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SpeechEnvDocker)",
   "language": "python",
   "name": "speechenvdocker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
