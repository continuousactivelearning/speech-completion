{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4538b65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "from hdbscan import approximate_predict\n",
    "\n",
    "import ruptures as rpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670beb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4031 rows across 101 videos\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/chunked_with_changepoints.csv\")\n",
    "\n",
    "def parse_embedding(s):\n",
    "    s_clean = s.strip(\"[]\")\n",
    "    return np.fromstring(s_clean, sep=\" \")\n",
    "\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(parse_embedding)\n",
    "print(f\"Loaded {len(df)} rows across {df['VideoTitle'].nunique()} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0742528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_enhanced_features(embeddings):\n",
    "    novelties = []\n",
    "    for i in range(1, len(embeddings)):\n",
    "        sim = cosine_similarity([embeddings[i]], [embeddings[i-1]])[0][0]\n",
    "        novelties.append(1 - sim)\n",
    "\n",
    "    if not novelties:\n",
    "        return [0] * 10\n",
    "\n",
    "    mean_novelty = np.mean(novelties)\n",
    "    var_novelty = np.var(novelties)\n",
    "    max_novelty = np.max(novelties)\n",
    "    min_novelty = np.min(novelties)\n",
    "\n",
    "    try:\n",
    "        trend_novelty = np.polyfit(range(len(novelties)), novelties, 1)[0]\n",
    "    except:\n",
    "        trend_novelty = 0\n",
    "\n",
    "    recent_window = min(5, len(novelties))\n",
    "    recent_novelty = np.mean(novelties[-recent_window:]) if recent_window > 0 else 0\n",
    "\n",
    "    num_cps = 0\n",
    "    cp_density = 0\n",
    "    recent_cp_activity = 0\n",
    "    cp_recency = 0\n",
    "\n",
    "    if len(embeddings) >= 5:\n",
    "        try:\n",
    "            novelty_array = np.array(novelties).reshape(-1, 1)\n",
    "            change_points = []\n",
    "\n",
    "            model_cp = rpt.Pelt(model=\"l2\").fit(novelty_array)\n",
    "            cps = model_cp.predict(pen=3)\n",
    "            if len(cps) > 1:\n",
    "                change_points.extend(cps[:-1])\n",
    "\n",
    "            novelty_threshold = np.mean(novelties) + 1.5 * np.std(novelties)\n",
    "            threshold_cps = [i for i, nov in enumerate(novelties) if nov > novelty_threshold]\n",
    "            change_points.extend(threshold_cps)\n",
    "\n",
    "            change_points = sorted(list(set(change_points)))\n",
    "            change_points = [cp for cp in change_points if 0 < cp < len(embeddings)]\n",
    "\n",
    "            num_cps = len(change_points)\n",
    "            if num_cps > 0:\n",
    "                cp_density = num_cps / len(embeddings)\n",
    "                recent_threshold = int(0.7 * len(embeddings))\n",
    "                recent_cps = sum(1 for cp in change_points if cp > recent_threshold)\n",
    "                chunks_in_recent = len(embeddings) - recent_threshold\n",
    "                recent_cp_activity = recent_cps / max(1, chunks_in_recent)\n",
    "                last_cp = max(change_points)\n",
    "                cp_recency = (len(embeddings) - last_cp) / len(embeddings)\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return [\n",
    "        mean_novelty, var_novelty, max_novelty, min_novelty,\n",
    "        trend_novelty, recent_novelty, num_cps, cp_density,\n",
    "        recent_cp_activity, cp_recency\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec5f421-4256-448b-8545-e2b1fde4b5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: (3526, 10), Targets: (3526,), Text Chunks: (3526, 2)\n"
     ]
    }
   ],
   "source": [
    "feature_rows = []\n",
    "target_rows = []\n",
    "raw_chunks = []\n",
    "\n",
    "for video_title, group in df.groupby(\"VideoTitle\"):\n",
    "    group = group.reset_index(drop=True)\n",
    "    embeddings = list(group[\"embedding\"])\n",
    "    total_chunks = len(group)\n",
    "\n",
    "    for i in range(5, total_chunks):\n",
    "        feats = extract_enhanced_features(embeddings[:i])\n",
    "        feature_rows.append(feats)\n",
    "        target_rows.append((i / total_chunks) * 100)\n",
    "\n",
    "        # Store both Chunk and Speech_ID\n",
    "        raw_chunks.append({\n",
    "            \"Chunk\": group[\"Chunk\"].iloc[i - 1],\n",
    "            \"Speech_ID\": video_title\n",
    "        })\n",
    "\n",
    "X = np.array(feature_rows)\n",
    "y = np.array(target_rows)\n",
    "X_df = pd.DataFrame(raw_chunks)\n",
    "\n",
    "print(f\"Features: {X.shape}, Targets: {y.shape}, Text Chunks: {X_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "522dd272-282c-42b5-8b4f-514f707072cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val, X_df_train, X_df_val = train_test_split(\n",
    "    X_scaled, y, X_df, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71bd2e99-b36b-4ce6-80fc-0a1c20327066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom transformers created successfully.\n"
     ]
    }
   ],
   "source": [
    "from hdbscan import approximate_predict \n",
    "\n",
    "class TextFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract structural features from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for text in X:\n",
    "            words = text.split()\n",
    "            num_words = len(words)\n",
    "            num_chars = len(text)\n",
    "            num_commas = text.count(',')\n",
    "            num_periods = text.count('.')\n",
    "            num_exclaims = text.count('!')\n",
    "            num_questions = text.count('?')\n",
    "            unique_words = len(set(words))\n",
    "            fraction_unique_words = unique_words / (num_words + 1e-5)\n",
    "            \n",
    "            features.append([\n",
    "                num_words, num_chars, num_commas, num_periods, \n",
    "                num_exclaims, num_questions, unique_words, fraction_unique_words\n",
    "            ])\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return ['num_words', 'num_chars', 'num_commas', 'num_periods', \n",
    "                'num_exclaims', 'num_questions', 'unique_words', 'fraction_unique_words']\n",
    "\n",
    "\n",
    "class EmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Generate sentence embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"all-mpnet-base-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.model is None:\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "        return self.model.encode(X.tolist(), show_progress_bar=True)\n",
    "\n",
    "\n",
    "class ClusteringTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Apply PCA, UMAP, and clustering to embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, n_pca_components=5, n_umap_components=2):\n",
    "        self.n_pca_components = n_pca_components\n",
    "        self.n_umap_components = n_umap_components\n",
    "        self.pca = PCA(n_components=n_pca_components, random_state=42)\n",
    "        self.umap_model = umap.UMAP(n_neighbors=15, min_dist=0.0, \n",
    "                                    n_components=n_umap_components, random_state=42)\n",
    "        self.clusterer = hdbscan.HDBSCAN(min_cluster_size=5, prediction_data=True)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        pca_features = self.pca.fit_transform(X)\n",
    "        umap_features = self.umap_model.fit_transform(X)\n",
    "        self.clusterer.fit(umap_features)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        pca_features = self.pca.transform(X)\n",
    "        umap_features = self.umap_model.transform(X)\n",
    "\n",
    "        cluster_labels, _ = approximate_predict(self.clusterer, umap_features)\n",
    "        is_noise = (cluster_labels == -1).astype(int)\n",
    "\n",
    "        features = np.column_stack([pca_features, cluster_labels, is_noise])\n",
    "        return features, cluster_labels\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        pca_names = [f'pca_{i+1}' for i in range(self.n_pca_components)]\n",
    "        return pca_names + ['cluster', 'is_noise']\n",
    "\n",
    "print(\"Custom transformers created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab416dde-729a-4896-9d71-d890e0c2881d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single unified model class created successfully.\n"
     ]
    }
   ],
   "source": [
    "class SpeechCompletionPredictor(BaseEstimator):\n",
    "    \"\"\"Single unified model for speech completion prediction\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.text_extractor = TextFeatureExtractor()\n",
    "        self.embedding_transformer = EmbeddingTransformer()\n",
    "        self.clustering_transformer = ClusteringTransformer()\n",
    "        self.lgb_model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.01, random_state=42)\n",
    "        self.feature_names = None\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def _compute_temporal_features(self, df_temp, cluster_labels):\n",
    "        \"\"\"Compute temporal cluster features\"\"\"\n",
    "        cluster_progress = []\n",
    "        fraction_unique_clusters = []\n",
    "\n",
    "        for _, group in df_temp.groupby(\"Speech_ID\"):\n",
    "            seen = set()\n",
    "            total_clusters = set(group['cluster'].unique())\n",
    "            cluster_seen = []\n",
    "            frac_unique = []\n",
    "\n",
    "            for c in group['cluster']:\n",
    "                cluster_seen.append(int(c in seen))\n",
    "                seen.add(c)\n",
    "                frac_unique.append(len(seen) / len(total_clusters))\n",
    "\n",
    "            cluster_progress.extend(cluster_seen)\n",
    "            fraction_unique_clusters.extend(frac_unique)\n",
    "\n",
    "        return np.array(cluster_progress), np.array(fraction_unique_clusters)\n",
    "\n",
    "    def _prepare_features(self, X_df):\n",
    "        \"\"\"Prepare all features for training/prediction\"\"\"\n",
    "        text_features = self.text_extractor.transform(X_df['Chunk'])\n",
    "        embeddings = self.embedding_transformer.transform(X_df['Chunk'])\n",
    "        clustering_features, cluster_labels = self.clustering_transformer.transform(embeddings)\n",
    "\n",
    "        df_temp = X_df.copy()\n",
    "        df_temp['cluster'] = cluster_labels\n",
    "        cluster_seen, frac_unique = self._compute_temporal_features(df_temp, cluster_labels)\n",
    "\n",
    "        all_features = np.column_stack([\n",
    "            text_features,\n",
    "            clustering_features,\n",
    "            cluster_seen.reshape(-1, 1),\n",
    "            frac_unique.reshape(-1, 1)\n",
    "        ])\n",
    "\n",
    "        return all_features\n",
    "\n",
    "    def fit(self, X_df, y, eval_set=None):\n",
    "        \"\"\"Fit the complete model\"\"\"\n",
    "        text_features = self.text_extractor.fit_transform(X_df['Chunk'])\n",
    "        embeddings = self.embedding_transformer.fit_transform(X_df['Chunk'])\n",
    "        clustering_features, cluster_labels = self.clustering_transformer.fit_transform(embeddings)\n",
    "\n",
    "        df_temp = X_df.copy()\n",
    "        df_temp['cluster'] = cluster_labels\n",
    "        cluster_seen, frac_unique = self._compute_temporal_features(df_temp, cluster_labels)\n",
    "\n",
    "        all_features = np.column_stack([\n",
    "            text_features,\n",
    "            clustering_features,\n",
    "            cluster_seen.reshape(-1, 1),\n",
    "            frac_unique.reshape(-1, 1)\n",
    "        ])\n",
    "\n",
    "        self.feature_names = (\n",
    "            self.text_extractor.get_feature_names_out() +\n",
    "            self.clustering_transformer.get_feature_names_out() +\n",
    "            ['cluster_seen_before', 'fraction_unique_clusters']\n",
    "        )\n",
    "\n",
    "\n",
    "        if eval_set is not None:\n",
    "            X_val_df, y_val = eval_set\n",
    "            val_features = self._prepare_features(X_val_df)\n",
    "            eval_set_processed = [(val_features, y_val)]\n",
    "\n",
    "            self.lgb_model.fit(\n",
    "                all_features, y,\n",
    "                eval_set=eval_set_processed,\n",
    "                eval_metric='mae',\n",
    "                callbacks=[early_stopping(stopping_rounds=50), log_evaluation(50)]\n",
    "            )\n",
    "        else:\n",
    "            self.lgb_model.fit(all_features, y)\n",
    "\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_df):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "\n",
    "        features = self._prepare_features(X_df)\n",
    "        return self.lgb_model.predict(features)\n",
    "\n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Get feature importance\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before getting feature importance\")\n",
    "\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': self.feature_names,\n",
    "            'importance': self.lgb_model.feature_importances_ / self.lgb_model.feature_importances_.sum()\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "        return importance_df\n",
    "\n",
    "print(\"Single unified model class created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b156a5f-e440-4c61-acbe-84f16c817441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n",
      "File size: 445766314 bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_file = \"../model/speech_completion_clustering_model.pkl\"\n",
    "\n",
    "print(\"File exists:\", os.path.exists(model_file))\n",
    "print(\"File size:\", os.path.getsize(model_file), \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e571be11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Load Random Forest model (semantic + structural features)\n",
    "rf_model = joblib.load(\"../model/tuned_random_forest_model.pkl\")\n",
    "\n",
    "# Load clustering-based model (encapsulates its own feature pipeline)\n",
    "cluster_model = joblib.load(\"../model/speech_completion_clustering_model.pkl\")\n",
    "\n",
    "# Load BERTopic model and completion regressor trained on topic-based saturation features\n",
    "topic_model = BERTopic.load(\"../model/final_bertopic_model\")\n",
    "completion_model = joblib.load(\"../model/completion_predictor.pkl\")\n",
    "\n",
    "print(\"All models loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6b4e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████| 23/23 [00:10<00:00,  2.16it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest (semantic + structural features)\n",
    "pred1 = rf_model.predict(X_val)\n",
    "\n",
    "# Clustering-based Model\n",
    "pred2 = cluster_model.predict(X_df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922fac7-fd51-40e4-8d3a-61a6beba523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Total Chunks per video\n",
    "video_lengths = X_df_val.groupby(\"Speech_ID\").agg({\n",
    "    \"Chunk\": \"count\"\n",
    "}).rename(columns={\"Chunk\": \"Total_Chunks\"}).reset_index()\n",
    "\n",
    "# Simulate Saturation Chunk (e.g., midpoint, unless you stored actual)\n",
    "video_lengths[\"Saturation_Chunk_Position\"] = video_lengths[\"Total_Chunks\"] // 2\n",
    "\n",
    "# Predict estimated completion percentage\n",
    "X_topic = video_lengths[[\"Saturation_Chunk_Position\", \"Total_Chunks\"]]\n",
    "video_lengths[\"Predicted_Saturation\"] = completion_model.predict(X_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b531d-1cdb-4688-ae1e-7f90fcc2f85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Model MAE: 0.7585%\n",
      "pred3 shape: (706,)\n",
      "y_true shape: (706,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "df_topic = pd.read_csv(\"../data/processed/final_topic_modeling.csv\")\n",
    "\n",
    "# Dropping any potential duplicates \n",
    "df_topic = df_topic.drop_duplicates(subset=[\"VideoTitle\", \"Chunk_ID\"])\n",
    "\n",
    "# Preparation of X_df_val with Chunk_ID\n",
    "X_df_val_temp = X_df_val.copy()\n",
    "X_df_val_temp[\"Chunk_ID\"] = X_df_val_temp.groupby(\"Speech_ID\").cumcount()\n",
    "X_df_val_temp = X_df_val_temp.rename(columns={\"Speech_ID\": \"VideoTitle\"})\n",
    "\n",
    "# Ensure types match\n",
    "X_df_val_temp[\"Chunk_ID\"] = X_df_val_temp[\"Chunk_ID\"].astype(int)\n",
    "df_topic[\"Chunk_ID\"] = df_topic[\"Chunk_ID\"].astype(int)\n",
    "\n",
    "# Merge cleanly\n",
    "X_val_topic = X_df_val_temp.merge(\n",
    "    df_topic[[\"VideoTitle\", \"Chunk_ID\", \"Saturation_Chunk_Position\", \"Total_Chunks\", \"Estimated_Completion_%\"]],\n",
    "    on=[\"VideoTitle\", \"Chunk_ID\"],\n",
    "    how=\"inner\"  # ensures only valid matches\n",
    ")\n",
    "\n",
    "# Predict\n",
    "topic_features = X_val_topic[[\"Saturation_Chunk_Position\", \"Total_Chunks\"]].fillna(0)\n",
    "pred3 = completion_model.predict(topic_features)\n",
    "\n",
    "# True values\n",
    "y_true = X_val_topic[\"Estimated_Completion_%\"].values\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Topic Model MAE: {mean_absolute_error(y_true, pred3):.4f}%\")\n",
    "print(f\"pred3 shape: {pred3.shape}\")\n",
    "print(f\"y_true shape: {y_true.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b28a8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MAE: 5.9796\n",
      "Clustering Model MAE: 22.8508\n",
      "Topic Model MAE: 0.7585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Evaluation of Random Forest and Clustering using y_val (original target)\n",
    "mae_rf = mean_absolute_error(y_val, pred1)\n",
    "mae_cluster = mean_absolute_error(y_val, pred2)\n",
    "\n",
    "# Evaluation of Topic Model using y_true (from df_topic)\n",
    "mae_topic = mean_absolute_error(y_true, pred3)\n",
    "\n",
    "# Print MAEs\n",
    "print(f\"Random Forest MAE: {mae_rf:.4f}\")\n",
    "print(f\"Clustering Model MAE: {mae_cluster:.4f}\")\n",
    "print(f\"Topic Model MAE: {mae_topic:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e5426f-d7d4-402b-8a4d-a1b41d3e86f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constrained Best Weights:\n",
      "Random Forest (w_rf): 0.0500\n",
      "Clustering     (w_cluster): 0.0100\n",
      "Topic Model    (w_topic): 0.9400\n",
      "\n",
      "Final Blended MAE: 2.9020\n",
      "Weights saved to ../model/ensemble_weights.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "best_mae = float(\"inf\")\n",
    "best_weights = (0.1, 0.02, 0.88)  # starting guess\n",
    "\n",
    "step = 0.01\n",
    "w1_min, w2_min = 0.05, 0.01\n",
    "\n",
    "for w1 in np.arange(w1_min, 1.0, step):\n",
    "    for w2 in np.arange(w2_min, 1.0 - w1 + step, step):\n",
    "        w3 = 1.0 - w1 - w2\n",
    "        if w3 < 0:\n",
    "            continue\n",
    "\n",
    "        ensemble_pred = w1 * pred1 + w2 * pred2 + w3 * pred3\n",
    "        mae = mean_absolute_error(y_true, ensemble_pred)\n",
    "\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_weights = (w1, w2, w3)\n",
    "\n",
    "# Extract final weights\n",
    "w1, w2, w3 = best_weights\n",
    "\n",
    "print(\"Constrained Best Weights:\")\n",
    "print(f\"Random Forest (w_rf): {w1:.4f}\")\n",
    "print(f\"Clustering     (w_cluster): {w2:.4f}\")\n",
    "print(f\"Topic Model    (w_topic): {w3:.4f}\")\n",
    "print(f\"\\nFinal Blended MAE: {best_mae:.4f}\")\n",
    "\n",
    "with open(\"../model/ensemble_weights.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"w_rf\": w1,\n",
    "        \"w_cluster\": w2,\n",
    "        \"w_topic\": w3\n",
    "    }, f)\n",
    "\n",
    "print(\"Weights saved to ../model/ensemble_weights.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2546d0d9-566a-4083-aaff-82a22765ab0d",
   "metadata": {},
   "source": [
    "## Final Model Summary: Evaluation, Ensemble, and Feature Design\n",
    "\n",
    "### Individual Model Performance (Validation Set)\n",
    "\n",
    "| Model             | MAE (%) |\n",
    "|------------------|---------|\n",
    "| Topic Model       | 0.76    |\n",
    "| Random Forest     | 5.98    |\n",
    "| Clustering Model  | 22.85   |\n",
    "\n",
    "---\n",
    "\n",
    "### Ensemble Weight Optimization\n",
    "\n",
    "To combine the strengths of all three models, a **constrained weighted ensemble** was constructed using the formulation:\n",
    "\n",
    "final_prediction = w₁ * RF_prediction + w₂ * Clustering_prediction + w₃ * Topic_prediction\n",
    "\n",
    "\n",
    "After experimenting with unconstrained and constrained optimization, the best validation performance was achieved with the following smart-blending weights:\n",
    "\n",
    "- **w₁ = 0.0500** (Random Forest)  \n",
    "- **w₂ = 0.0100** (Clustering Model)  \n",
    "- **w₃ = 0.9400** (Topic Model)\n",
    "\n",
    "**Final Blended MAE:** **2.9020%**\n",
    "\n",
    "This formulation ensured that all models contributed meaningfully, while allowing the topic model to dominate due to its superior accuracy.\n",
    "\n",
    "Weights saved to: `../model/ensemble_weights.json`\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Design Philosophy\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "The Random Forest model used **explicit, structured features**, including:\n",
    "\n",
    "- Mean and variance of semantic novelty  \n",
    "- Textual complexity indicators  \n",
    "- Number and density of semantic change points  \n",
    "- Relative recency of the last detected change  \n",
    "\n",
    "These hand-engineered features made the model interpretable and structured, but its accuracy plateaued when handling nuanced conceptual shifts.\n",
    "\n",
    "#### Clustering-Based Model\n",
    "\n",
    "The clustering model used an **end-to-end unsupervised pipeline** involving:\n",
    "\n",
    "- Sentence embeddings  \n",
    "- PCA + UMAP for semantic compression  \n",
    "- HDBSCAN clustering  \n",
    "- Change dynamics over cluster labels  \n",
    "\n",
    "While this helped track latent structure evolution, it was sensitive to noise and had limited predictive reliability.\n",
    "\n",
    "#### Topic Model-Based Completion Estimator\n",
    "\n",
    "The BERTopic-based model inferred progress using **topic saturation**:\n",
    "\n",
    "- Assigned topic labels to each chunk  \n",
    "- Counted cumulative unique topics over time  \n",
    "- Detected saturation (no new topics)  \n",
    "- Inferred completion % from saturation point  \n",
    "\n",
    "This method aligned naturally with how concepts evolve in a speech, leading to both stability and high accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Decision\n",
    "\n",
    "The final system uses a **blended ensemble combining structured, clustering-based, and topic modeling predictions**.  \n",
    "This ensemble balances generalization and robustness, achieving a **final MAE of 2.90%**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
